{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU ID\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheongwoong/miniconda3/envs/iti/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "MODELS = {\n",
    "    'llama3_8B': 'meta-llama/Meta-Llama-3-8B',\n",
    "    'llama3_8B_instruct': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "}\n",
    "\n",
    "model_name = 'llama3_8B'\n",
    "MODEL = MODELS[model_name]\n",
    "\n",
    "model = LanguageModel(MODEL, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect activations and compute direction\n",
    "prompt = \"I talk about weddings constantly\"\n",
    "with model.trace(prompt) as tracer:\n",
    "    hidden_states = [model.model.layers[i].output[0].squeeze().detach().cpu().save() for i in range(model.config.num_hidden_layers)]\n",
    "    head_wise_hidden_states = [model.model.layers[i].self_attn.o_proj.input.squeeze().detach().cpu().save() for i in range(model.config.num_hidden_layers)]\n",
    "hidden_states = torch.stack(hidden_states, dim = 0).squeeze().numpy()\n",
    "head_wise_hidden_states = torch.stack(head_wise_hidden_states, dim = 0).squeeze().numpy()\n",
    "pos = head_wise_hidden_states[:,-1,:]\n",
    "\n",
    "prompt = \"I do not talk about weddings constantly\"\n",
    "with model.trace(prompt) as tracer:\n",
    "    hidden_states = [model.model.layers[i].output[0].squeeze().detach().cpu().save() for i in range(model.config.num_hidden_layers)]\n",
    "    head_wise_hidden_states = [model.model.layers[i].self_attn.o_proj.input.squeeze().detach().cpu().save() for i in range(model.config.num_hidden_layers)]\n",
    "hidden_states = torch.stack(hidden_states, dim = 0).squeeze().numpy()\n",
    "head_wise_hidden_states = torch.stack(head_wise_hidden_states, dim = 0).squeeze().numpy()\n",
    "neg = head_wise_hidden_states[:,-1,:]\n",
    "\n",
    "direction = pos - neg\n",
    "direction = torch.from_numpy(direction).to('cuda')\n",
    "direction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: I went up to my friend and said, \"I'm sorry, but I have to tell you something. I'm a lesbian.\"\n",
      "She looked at me and said, \"I'm sorry, but I have to tell you something. I'm a lesbian too.\"\n",
      "I said, \"Oh, no, I'm not a lesbian. I'm just gay.\"\n",
      "\n",
      "\n",
      "ITI: I went up to my friend and said, “I’m going to be a dad, too!” I couldn’t wait to tell him about my plans, and about the baby that was on the way. I was so excited. But there was one problem. I didn’t have a place to stay, I was about to turn 18. But I was\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I went up to my friend and said\"\n",
    "with model.generate(prompt, max_new_tokens=64, do_sample=False) as tracer:\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        model.model.layers[i].self_attn.o_proj.input[:,-1,:] += direction[i]\n",
    "    for _ in range(64 - 1):\n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "            model.model.layers[i].self_attn.o_proj.next().input[:,-1,:] += direction[i]\n",
    "    iti_response = model.generator.output.save()\n",
    "\n",
    "with model.generate(prompt, max_new_tokens=64, do_sample=False) as tracer:\n",
    "    original_response = model.generator.output.save()\n",
    "\n",
    "print('Baseline:', model.tokenizer.decode(original_response[0], skip_special_tokens=True))\n",
    "print()\n",
    "print('ITI:', model.tokenizer.decode(iti_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
